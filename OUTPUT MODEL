#OUTPUT MODEL, ALL BINARY LOGIC AND FACTORS ARE BASED ON THE USER'S INFORMATION FROM INTERNET READING COUNTLESS ARTICLES. Inspired by Litchman's model. 


import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')
import json
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score

import tensorflow as tf
from tensorflow.keras.layers import Input, Dense, Dropout
from tensorflow.keras.models import Model
from scipy.stats import ttest_1samp

class EconomicTrendsProcessor:
    def __init__(self, csv_path="economic_trends_correlations_feature_scores.csv"):
        self.csv_path = csv_path
        self.scaler = StandardScaler()
        self.feature_vector = []
        self.score_matrix = np.array([])

    def load_correlation_data(self):
        d_df = pd.read_csv(self.csv_path)
        numeric_df = d_df.select_dtypes(include=[np.number]).apply(pd.to_numeric).fillna(0)
        full_features = numeric_df.to_numpy().flatten()
        self.scorematrix = numeric_df.to_numpy()
        self.scorematrix = numeric_df.iloc[:5, :10].to_numpy()
        self.feature_vector = self.scorematrix.tolist()

    def displaydata(self):
        print(self.scorematrix.shape)
        print(self.score_matrix)
        print(self.feature_vector)

class NewsSentimentProcessor:
    def __init__(self, csv_path="neutral_texts (1).csv"):
        self.csv_path = csv_path
        self.scaler = StandardScaler()
        self.feature_vector = []
        self.score_matrix = np.array([])

    def load_correlation_data(self):
        df = pd.read_csv(self.csv_path)
        s_df = df.sample(n=391, random_state=42)

        def parse_sentiment(sentiment_str):
            try:
                data = json.loads(sentiment_str)
                return data.get('compound')
            except:
                return np.nan

        s_df['parsed_score'] = s_df['sentiment_scores'].apply(parse_sentiment)
        s_df['parsed_score'] = s_df['parsed_score'].fillna(0)
        self.score_columns = ['parsed_score']
        self.scorematrix = s_df[self.score_columns].to_numpy().astype(float)
        self.feature_vector = self.scorematrix.tolist()

    def displaydata(self):
        print(self.scorematrix.shape)
        print(self.score_matrix)
        print(self.feature_vector)

class DemographicsTrendsProcessor:
    def __init__(self, csv_path="demographics_trends_correlations_full.csv"):
        self.csv_path = csv_path
        self.scaler = StandardScaler()
        self.feature_vector = []
        self.score_matrix = np.array([])

    def load_correlation_data(self):
        df = pd.read_csv(self.csv_path)
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        full_features = df[numeric_cols].to_numpy().flatten()
        self.scorematrix = full_features[:125].reshape(5, 25)
        self.feature_vector = self.scorematrix.tolist()

    def displaydata(self):
        print(self.scorematrix.shape)
        print(self.score_matrix)
        print(self.feature_vector)

factors = [
    "Party_Leadership", "Party_Popularity", "Economic_Criticism",
    "Opposition_Polling_Strength", "Economy", "Redistricting_Impact",
    "Social_Momentum", "Foreign_Policy_Critique", "Voter_Engagement",
    "Incumbent_party_advantage", "Media_Sentiment_Favorability"
]

y_factors_hist = np.array([
    [1,0,0,0,1,0,0,1,1,0,0],
    [0,0,1,1,0,0,1,0,1,0,1],
    [0,0,0,1,1,0,0,1,0,0,1],
    [0,0,0,0,0,1,1,0,1,0,0],
    [1,0,1,0,1,0,0,1,0,0,1]
])
y_winner_hist = np.array([0, 0, 1, 1, 0]).reshape(-1, 1)
y_train = np.hstack([y_factors_hist, y_winner_hist])

econ = EconomicTrendsProcessor()
econ.load_correlation_data()

news = NewsSentimentProcessor()
news.load_correlation_data()

demo = DemographicsTrendsProcessor()
demo.load_correlation_data()

samples = 5
FVL = 419
x_historical_features = []

all_news_features = news.scorematrix.flatten().tolist()

for i in range(samples):
    econ_features = econ.feature_vector[i]
    demo_features = demo.feature_vector[i]

    news_features = all_news_features

    x_historical_features.append(np.array([
        *econ_features,
        *news_features,
        *demo_features
    ]))

X_train = np.array(x_historical_features)

i_pred = samples - 1

x = np.array([
    *econ.feature_vector[i_pred],
    *all_news_features,
    *demo.feature_vector[i_pred]
]).reshape(1, -1)

X_2024 = x

def build_model():
    inputs = Input(shape=(419,))
    x = Dense(64, activation='relu')(inputs)
    x = Dropout(0.4)(x)
    x = Dense(32, activation='relu')(x)
    x = Dropout(0.3)(x)
    outputs = Dense(12, activation='sigmoid')(x)
    model = Model(inputs, outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

def crossvalidatingstuff(X, y):
    tscv = TimeSeriesSplit(n_splits=4)
    results = []
    for train_idx, test_idx in tscv.split(X):
        X_tr, X_te = X[train_idx], X[test_idx]
        y_tr, y_te = y[train_idx], y[test_idx]
        model = build_model()
        model.fit(X_tr, y_tr, batch_size=2, epochs=15, verbose=0)
        pred = model.predict(X_te, verbose=0)
        pred_bin = (pred > 0.5).astype(int)
        mae = mean_absolute_error(y_te, pred)
        f1 = f1_score(y_te.flatten(), pred_bin.flatten(), average='weighted')
        acc = accuracy_score(y_te.flatten(), pred_bin.flatten())
        mape = (1 - acc) * 100
        results.append({'MAE': mae, 'F1_score': f1, 'Test_accuracy': acc, 'Test_mape': mape})
    return pd.DataFrame(results)

def metrictimeyahoo(y_true, y_pred):
    pred_bin = (y_pred > 0.5).astype(int)
    mse = mean_squared_error(y_true, y_pred)
    mae = mean_absolute_error(y_true, y_pred)
    f1 = f1_score(y_true.flatten(), pred_bin.flatten(), average='weighted')
    acc = accuracy_score(y_true.flatten(), pred_bin.flatten())
    mape = (1 - acc) * 100
    return {'MSE': mse, 'MAE': mae, 'F1': f1, 'MAPE': mape}

cv_df = crossvalidatingstuff(X_train, y_train)

n_simulations = 100
all_factor_probs = []
all_winners_pred = []
sim_metrics_list = []

for i in range(1, n_simulations + 1):
    tf.random.set_seed(i)
    np.random.seed(i)
    model = build_model()
    model.fit(X_train, y_train, batch_size=2, epochs=15, verbose=0)
    pred = model.predict(X_2024, verbose=0)[0]
    factor_probs = pred[:11]
    if pred[11] > 0.5:
      winner_pred = 1
    else:
      winner_pred =0
    all_factor_probs.append(factor_probs)
    all_winners_pred.append(winner_pred)
    metrics = metrictimeyahoo(y_train, model.predict(X_train, verbose=0))
    sim_metrics_list.append(metrics)

f_df = pd.DataFrame(sim_metrics_list)
all_factor_probs = np.array(all_factor_probs)
all_winners_pred = np.array(all_winners_pred)

print(f"Overall Prediction Accuracy: {np.mean(all_winners_pred == 0)*100:.2f}%")
print(f"Mean Mean MSE: {f_df['MSE'].mean():.4f}")
print(f"Mean Mean MAE: {f_df['MAE'].mean():.4f}")
print(f"Mean Mean F1 Score: {f_df['F1'].mean():.4f}")
print(f"Mean Mean MAPE: {f_df['MAPE'].mean():.4f}")
print(f"Mean test accuracy: {cv_df['Test_accuracy'].mean():.4f}")
print(f"Mean CV MAE: {cv_df['MAE'].mean():.4f}")
print(f"Mean CV F1 Score: {cv_df['F1_score'].mean():.4f}")
print(f"Mean CV MAPE: {cv_df['Test_mape'].mean():.4f}")

for i, factor in enumerate(factors):
    probs = all_factor_probs[:, i]
    M = np.mean(probs)
    SD = np.std(probs, ddof=1)
    t_stat, p_val = ttest_1samp(probs, popmean=0.5)
    df = n_simulations - 1
    if p_val < 0.001:
        p_text = "p < .001"
    else:
        p_text = f"p = {p_val:.3f}"
    print(f"{factor} M = {M:.3f}, SD = {SD:.3f}, t({df}) = {t_stat:.2f}, {p_text}")
#Running main. 
    main()
